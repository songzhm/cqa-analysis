from nltk.tokenize.stanford_segmenter import StanfordSegmenter
import nltk

nltk.internals.config_java("C:\Program Files (x86)\Java\jre1.8.0_101\bin\java.exe")

segmenter = StanfordSegmenter(
    path_to_jar="C:\\Users\\MSI CES 2014\Anaconda3\\Lib\\site-packages\\nltk\\tokenize\\stanford-segmenter-2015-12-09\\stanford-segmenter-3.6.0.jar",
    path_to_slf4j = "C:\\Users\\MSI CES 2014\Anaconda3\\Lib\\site-packages\\nltk\\tokenize\\stanford-segmenter-2015-12-09\\slf4j-api.jar",
    path_to_sihan_corpora_dict="C:\\Users\\MSI CES 2014\\Anaconda3\\Lib\\site-packages\\nltk\\tokenize\\stanford-segmenter-2015-12-09\\data",
    path_to_model="C:\\Users\\MSI CES 2014\\Anaconda3\\Lib\\site-packages\\nltk\\tokenize\\stanford-segmenter-2015-12-09\\data\\pku.gz",
    path_to_dict="C:\\Users\\MSI CES 2014\\Anaconda3\\Lib\\site-packages\\nltk\\tokenize\\stanford-segmenter-2015-12-09\\data\\dict-chris6.ser.gz")

sentence = u"这是斯坦福中文分词器测试"
segmenter.segment(sentence)
